# ğŸ§  Toy Qwen3MoE - Character-Level LLM

A **toy implementation of Qwen3MoE architecture** for learning purposes. This project trains a character-level language model using a simplified version of the Qwen3MoE transformer architecture. The model learns to generate Shakespeare-like text based on input prompts.

> âš ï¸ **Disclaimer**: This project is for **educational purposes only**. It is a minimal and experimental implementation. There might be inaccuracies, oversights, or suboptimal decisions. Please feel free to open issues or share suggestions!

---

## âœ¨ Features

- Character-level tokenizer
- Configurable transformer architecture (Qwen3MoE-inspired)
- Loss plotted and saved over training epochs
- Generation script with support for temperature sampling
- MPS (Mac) and CUDA GPU support with `autocast` context

---

## ğŸ“ˆ Training Results

Trained for **24 epochs** on Shakespeare dataset (char-level) using MPS device. Final training loss: **~2.374**.

### ğŸ”¹ Training Output

<img src="assets/training.png" alt="Generated Text Example" width="600" />

### ğŸ”¹ Generated Text

<img src="assets/output.png" alt="Generated Text Example" width="600" />

### ğŸ”¹ Loss Curve

<img src="assets/loss_curve_24epochs.png" alt="Training Loss Curve" width="600" />

---

## ğŸ› ï¸ Usage

### 1. Train the model
```bash
uv run train.py
```

### 2. Generate text
```bash
uv run generate.py
```

---

## ğŸ“ Directory Structure

```
toy-qwen3MoE/
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ toy_qwen3MoE/
â”‚       â””â”€â”€ model.py          # Model architecture
â”‚
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ loss_curve_24epochs.png
â”‚
â”œâ”€â”€ train.py                  # Training loop
â”œâ”€â”€ generate.py               # Text generation
â”œâ”€â”€ char_vocab.json           # Character-level vocab
â”œâ”€â”€ toy_qwen3MoE_24epochs.pth    # Trained model weights (gitignored)
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ğŸ™ Acknowledgements

Thanks to the open-source community for resources and ideas. Special thanks to the Qwen team for releasing their architecture and inspiring learning projects like this.

---

## ğŸ“Œ Notes

- The model weights (`*.pth`) are excluded from the repo.
- Results may vary based on training duration and device.
- The implementation prioritizes readability and learning over efficiency.